{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generates a new test case based on the problem statement using GPT-2. * For the simple problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define paths\n",
    "# input_file_path = \"scraped/simple/simple-01.txt\"\n",
    "# output_dir = \"generated/gpt2-simple-1\"\n",
    "input_file_path = \"scraped/simple/simple-04.txt\"\n",
    "output_dir = \"generated/gpt2-simple-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse dataset\n",
    "def parse_problems(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read().strip()\n",
    "    \n",
    "    problems = content.split(\"===\")\n",
    "    parsed_data = []\n",
    "    \n",
    "    for idx, problem in enumerate(problems):\n",
    "        parts = problem.strip().split(\"### Test Case ###\")\n",
    "        if len(parts) < 2:\n",
    "            continue  # Skip malformed entries\n",
    "        \n",
    "        problem_statement = parts[0].strip()\n",
    "        test_cases = [tc.strip() for tc in parts[1:]]\n",
    "\n",
    "        parsed_data.append({\n",
    "            \"id\": idx + 1,\n",
    "            \"problem\": problem_statement,\n",
    "            \"test_cases\": test_cases,\n",
    "            \"raw\": problem,\n",
    "        })\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "# Process and generate new test cases\n",
    "problems = parse_problems(input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # or \"gpt2-small\"\n",
    "\n",
    "# Check if MPS is available\n",
    "# mps: 2.12s/it\n",
    "# cpu: 1.56s/it\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def generate_test_cases_batched(prompts, batch_size=64):\n",
    "    \"\"\"\n",
    "    Generate multiple test cases in parallel using a batch.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = tokenized_inputs[\"input_ids\"].to(device)  # Move batch to MPS\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=24, \n",
    "            num_return_sequences=1, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            # repetition_penalty=1.1\n",
    "        )\n",
    "\n",
    "    # Move back to CPU and decode\n",
    "    generated_texts = [tokenizer.decode(ids.cpu(), skip_special_tokens=True) for ids in output_ids]\n",
    "    return generated_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20 statements.\n"
     ]
    }
   ],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "prompts = [f\"{problem_data['raw']}\\n### Test Case ###\" for problem_data in problems]\n",
    "\n",
    "generated_cases = generate_test_cases_batched(prompts, batch_size=64)\n",
    "\n",
    "# Print results\n",
    "for i, updated in enumerate(generated_cases):\n",
    "    output_file_path = os.path.join(output_dir, f\"{problems[i]['id']}.txt\")\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(updated)\n",
    "\n",
    "print(f\"Saved {len(generated_cases)} statements.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
